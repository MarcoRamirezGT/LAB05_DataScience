---
title: "Informe"
author: "Marco Ramirez, Estuardo Hernandez"
date: "2022-09-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analisis Exploratorio de la columna categoria

```{r}
library(wordcloud2)
library(ggplot2)



db<- read.csv('train.csv')
# limpieza de datos

db$text<-gsub("#[A-Za-z0-9]+|@[A-Za-z0-9]+|\\w+(?:\\.\\w+)*/\\S+", "", db$text)
db$text<-toupper(db$text)

db$text<-gsub("HTTP://","",db$text)
db$text<-gsub("HTTPS://","",db$text)
db$text<-gsub("[^\x01-\x7F]", "",db$text)
db$text<-gsub("B&amp;N", "",db$text)
db$text<-gsub("#", "",db$text)
db$text<-gsub('(s+)(A|AN|AND|THE|I)(s+)', '', db$text)
db$text<-gsub(':', '', db$text)
db$text<-gsub("'", '', db$text)
db$text<-gsub("--|", '', db$text)
db$text<-gsub('[[:punct:]]', '', db$text)
db$id<-toupper(db$id)
db$keyword<-toupper(db$keyword)
db$location<-toupper(db$location)


db[db == ""]<-NA

# Frecuencia de los tweets categorizados como NO CATASTROFICOS
freq<-table(db$keyword, useNA = 'no')

# Nube de palabras de esa categoria
wordcloud2(data = freq, size = 0.15, shape = "cloud",
           color="random-dark", ellipticity = 0.5)
# Histograma
hist(x = freq, main = "Histograma De Palabras Más Repetidas", 
     xlab = "Frecuencia", ylab = "",
     col = "ivory")

no_disaster<-subset(x = db, subset = target == 0, select = c("keyword"))

freq_no_disaster<-table(no_disaster$keyword)
tabla_ordenada1<-freq_no_disaster[order(freq_no_disaster, decreasing = TRUE, na.last = TRUE)]
#View(tabla_ordenada1)
h1<-head(tabla_ordenada1)
h1<-as.data.frame(h1)

# Nube de palabras categorizadas como NO DESASTRES
wordcloud2(data = freq_no_disaster, size = 0.2, shape = "cloud",
           color="random-dark", ellipticity = 0.5)


# Grafica de las palabras claves de NO DESASTRES
ggplot(data=h1, aes(x=Var1, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=as.integer(Freq)), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="No Disaster Tweets",x='Palabra', y="Frecuencia")+
  theme(legend.position="none")

disaster<-subset(x = db, subset = target == 1, select = c("keyword"))

freq_disaster<-table(disaster$keyword)
tabla_ordenada2<-freq_disaster[order(freq_disaster, decreasing = TRUE, na.last = TRUE)]

h2<-head(tabla_ordenada2)
h2<-as.data.frame(h2)

# Grafica de las palabras claves categorizadas como DESASTRES
ggplot(data=h2, aes(x=Var1, y=Freq, fill=Var1)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=as.integer(Freq)), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Disaster Tweets",x='Palabra', y="Frecuencia")+
  theme(legend.position="none")
# Nube de palabras claves categorizadas como DESASTRES
wordcloud2(data = freq_disaster, size = 0.2, shape = "cloud",
           color="random-dark", ellipticity = 0.5)





```

Luego de limpiar los datos a trabajar y mostrar las nubes de palabras de la columna keyword de los tweets, tanto los que son de desastres reales y los que no, concluimos que la palabra que más frecuencia tiene en la categoría de tweets que no son considerados sobre desastres es "body bags" la cual aparece 40 veces. Luego, en la categoría de tweets que sí son desastres reales, la palabras con más frecuencia (39) fueron "derailment", "outbreak" y "wreckage", que en español son "descarrilamiento", "estallido", "destrucción", respectivamente.

También cabe mencionar que gracias al histograma de las palabras o keywords más repetidas tienen en su mayoría frecuencia entre el rango de 30 a 40 apariciones.

## Analisis exploratorio de la columna text

```{r}
### Analisis de texto
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")


db<- read.csv('train.csv')

# Limpieza del texto
db$text<-gsub("#[A-Za-z0-9]+|@[A-Za-z0-9]+|\\w+(?:\\.\\w+)*/\\S+", "", db$text)
db$text<-toupper(db$text)

db$text<-gsub("HTTP://","",db$text)
db$text<-gsub("HTTPS://","",db$text)
db$text<-gsub("[^\x01-\x7F]", "",db$text)
db$text<-gsub("B&amp;N", "",db$text)
db$text<-gsub("#", "",db$text)
db$text<-gsub('(s+)(A|AN|AND|THE|I)(s+)', '', db$text)
db$text<-gsub(':', '', db$text)
db$text<-gsub("'", '', db$text)
db$text<-gsub("--|", '', db$text)
db$text<-gsub('[[:punct:]]', '', db$text)
db$id<-toupper(db$id)
db$keyword<-toupper(db$keyword)
db$location<-toupper(db$location)


db[db == ""]<-NA

TextDoc <- Corpus(VectorSource(db$text))
#Replacing "/", "@" and "|" with space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
# Convert the text to lower case
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
# Remove numbers
TextDoc <- tm_map(TextDoc, removeNumbers)
# Remove english common stopwords
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
# Remove your own stop word
# specify your custom stopwords as a character vector
TextDoc <- tm_map(TextDoc, removeWords, c("like", "just", "get",'will','new','now','via','dont','one')) 
# Remove punctuations
TextDoc <- tm_map(TextDoc, removePunctuation)
# Eliminate extra white spaces
TextDoc <- tm_map(TextDoc, stripWhitespace)
# Text stemming - which reduces words to their root form
TextDoc <- tm_map(TextDoc, stemDocument)

TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
# Sort by descearing value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
# Display the top 5 most frequent words
head(dtm_d)
# Plot the most frequent words
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))
findAssocs(TextDoc_dtm, terms = c("fire","amp","bomb"), corlimit = 0.25)			

findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)

# regular sentiment score using get_sentiment() function and method of your choice
# please note that different methods may have different scales
syuzhet_vector <- get_sentiment(db$text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)

# bing
bing_vector <- get_sentiment(db$text, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector <- get_sentiment(db$text, method="afinn")
head(afinn_vector)
summary(afinn_vector)

#compare the first row of each vector using sign function
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)

d<-get_nrc_sentiment(db$text)
# head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe
head (d,10)


#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td[2:253]))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")


#Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)


```

Luego de realizar la limpieza adecuada para analizar los datos y mostrar los resultados, decimos que la palabra con más frecuencia que aparece en los tetos o contenido de los tweets es "fire" superando las 350 apariciones. Además, encontramos que tal palara tiene relación con las siguientes: "forest", "homasttopa", "nowwwwww", "truck" y "wild", lo cual hace mucho sentido porque son conceptos que hacen coherencia o tienen una fuerte vinculación en un contexto.

También se determinó que el sentimiento que más presencia tiene en los tweets es el miedo. Lo cual hace sentido porque es el primer sentimiento en llegar ante un desastre. La tristeza es el que le sigue y ambos abarcan un porcentaje considerable de todos los tweets analizados.

Las palabras que creemos que nos serán más útiles al momento de realizar el modelo de clasificación son las que tienen más frecuencia porque de algún modo nos indican que si un tweet tiene una palabra en concreto, tiene altas probabilidades de ser de una de las dos categorías dependiendo de la palabra que sea.

Valdría la pena explorar bigramas o trigramas para tener un estudio más complejo y encontrar qué palabras se relacionan con cuál y así tener más palabras que quizá puedan influir considerablemente en el modelo.

## Tweets más negativos y positivos

```{r}
db$Escala <- get_sentiment(db$text, method="bing")
#View(db)
# 
# 
db$Sentimiento[db$Escala>0]<-1
db$Sentimiento[db$Escala<0]<-0
# db$Sentimiento[db$Escala>-1 & db$Escala<1]<-'Neutro'

negativos<-db[order(db$Escala), ]
neg<-subset(x = negativos, select = c("id", "target", "Escala"))
neg2<-neg[1:10,]

g1<-ggplot(data=neg2, aes(x=id, y=Escala, fill=Escala)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=as.integer(Escala)), vjust=1.6, color="black",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Tweets más Negativos",x='Tweet', y="Negatividad")+
  theme(legend.position="none")

pos<-neg[7604:7613,]

g2<-ggplot(data=pos, aes(x=id, y=Escala, fill=Escala)) +
  geom_bar(stat="identity", position=position_dodge())+
  geom_text(aes(label=as.integer(Escala)), vjust=1.6, color="white",
            position = position_dodge(0.9), size=3.5)+
  labs(title="Tweets más Positivos",x='Tweet', y="Positividad")+
  theme(legend.position="none")

```

### ¿Cuáles son los 10 tweets más negativos? ¿En qué categoría están?

```{r}
g1

neg2
```

Gráfica con los 10 tweets más negativos. En el eje 'x' están los IDs de los tweets y en el eje 'y' está representada su negatividad conforme a la escala que definimos.

Y se observa que la mitad (5) de los tweets son de categoría "desastre real" y la otra mitad (5) de "no desastre".

### ¿Cuáles son los 10 tweets más positivos? ¿En qué categoría están?

```{r}
g2

pos
```

Gráfica con los 10 tweets más positivos. En el eje 'x' están los IDs de los tweets y en el eje 'y' está representada su positividad conforme a la escala que definimos.

Y se observa que la minoría (3) de los tweets son de categoría "desastre real" y el resto (7) de "no desastre".

### ¿Son los tweets de la categoría que indica que habla de un desastre real más negativos que los de la otra categoría?

Por las gráficas y preguntas planteadas anteriormente, se determina que los tweets de categoría de "desastre real" son más negativos que los tweets clasificados como "no desastre" a pesar de que los más negativos son mitad mitad pero en los tweets más positivos son más los de categoría "no desastre" por lo que afirmamos lo anterior, o se puede decir que los tweets de categoría "no desastre" son más positivos que los denominados "desastre real".
